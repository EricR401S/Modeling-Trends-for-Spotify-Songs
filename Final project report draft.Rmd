---
title: "EXPLORING POPULARITY AND EXPLICITY OF 2010s MUSIC THROUGH MUSICALITY."
author: "Eric Rios, Emma Wang, Pragya Raghuvanshi, Lorna Aine"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Load Libraries
library(psych)
library(ggplot2)
library(corrplot)
library(dplyr)
library(table1)
library(boot)
library(caret)
library(arm)
library(pROC)
library(e1071)
library(stargazer)
library(car)
library(liver)
library(jtools)
library(olsrr)
library(kableExtra)
```

```{r , echo = FALSE, message=FALSE, results = FALSE}
#read original data set
df <- read.csv("https://github.com/EricR401S/Modeling-Trends-for-Spotify-Songs/raw/main/archive/tracks.csv")
```

```{r datacleaning, echo = FALSE, message=FALSE, results = FALSE }

# Confirming the data types of the columns
sapply(df, class)

# Removing the brackets from the names of the artists
df$artists<-gsub("]","",as.character(df$artists))
df$artists<-gsub("^.","",as.character(df$artists))

# New minute variable for our own use to simplify interpretation
df$duration_minutes <- df$duration_ms/(1000*60)

# Confirming that there are no missing data, except the artist name column
colSums(is.na(df))

# change "explicit" into binary factor
df$explicit_fac <- factor(df$explicit,
                         levels=c(0,1),
                         labels=c('Non-Explicit','Explicit'))

# make Date readable to R
df$release_year <- substr(df$release_date, 1, 4)
df$release_year <- as.integer(df$release_year)

# According to variable definitions, speechiness levels above 0.66 are speech tracks such as podcasts and poetries.
df0 <- df[df$speechiness <= 0.66,]
nrow(df) - nrow(df0) #22,598 records of speech tracks

# Examining records with a value of 0 for tempo
# A total of 328 records with 0 tempo were found, and most were tracks of rain sounds and introductions.
sum(df0$tempo==0)

# 148 of those records are from the 2010s decade, our area of interest. 
tempo_0_subset <- df0[df0$tempo == 0,]
tempo_0_subset_2010s <- tempo_0_subset[grep('201[0-9].*', tempo_0_subset$release_date),]
nrow(tempo_0_subset_2010s)

# Removing records with a value of 0 for tempo
df0 <- df0[df0$tempo != 0,]
```

```{r , echo=FALSE,results="asis", header=FALSE, message=FALSE, warning=FALSE}
#subset data
subset <- df0[grepl('201[0-9].*', df0$release_year),]

RQ1_relation <- c("popularity", "acousticness", "danceability", "energy", "instrumentalness", "tempo", "loudness", "speechiness")
df1 = subset[RQ1_relation]

RQ2_relation <- c("explicit_fac", "danceability", "energy", "speechiness","tempo", "release_year")
df2 = subset[RQ2_relation]
df2$danceability <- df2$danceability * 100
df2$speechiness <- df2$speechiness * 100
df2$energy <- df2$energy  * 100

train <- df2[df2$release_year < 2018,]
test <- df2[df2$release_year >= 2018,]
keeps <- c("explicit_fac", "danceability", "energy", "speechiness", "tempo")
test <- test[keeps]
train <- train[keeps]
```

## Abstract

The rise of music streaming, algorithms and the ubiquity of ear buds and headphones in the 2010s decade forever changed the way in which we consume music. Beyond the feelings it invokes in us, music is a combination of different qualities that often cannot be easily discerned by the untrained human ear. In contrast, producers and artists, the ones with trained ears, brought forth musical innovations during this advent of technology that changed the way how different aspects of music are recorded, successfully adapting to the new streaming market. Spotify, for example, is the main streaming website that doubles as a gauge on what musical aspects, translated through a lens of streaming metrics, most influence popularity, as well as other indicators of a song's performance (Spotify, 2022). 

For this study, we subset a [Spotify dataset](https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks),with an assortment of descriptors focusing on the 2010s decade to investigate which qualities have been key to making songs popular and if they can be used to classify explicit and non explicit music, a phenomenon that has been rising during the past decade (Tayag, 2017). We found that x, y,z play a key role in determining the average popularity index of a song and x, y,z can potentially be used to predict of  a song is explicit or not.  

## Introduction

With over 456 million monthly active users as of September 2022, Spotify is one of the largest music streaming service providers. *Citation* that redefined the music streaming game by using algorithms to optimize user preferences and music stream monetization. This prompted music producers and creators to pay closer attention to the sonic characteristics of songs that drive more streams on the platform. Although Spotify's algorithm remains one of the unrivaled enigmas of music recommendation, the company builds large databases of tracks on the platform with granular details on their sonic characteristics synthesized by the algorithm. *It is with this information that we answered our research questions.* 

The data set used in this research contained `r format(nrow(subset), big.mark = ",")` of observations/ tracks and `r ncol(subset)` of variables for songs between 2010 and 2019 obtained from a larger [spotify dataset](https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks) that contained `r format(nrow(df), big.mark = ",")` tracks. These variables describe the sonic characteristics of songs and were defined below from Spotify's API documentation.

<ul>
-**Popularity** is calculated by an algorithm that is based on how many times a track has been played and how recent those plays were. This is the response variable of interest for research question 1 (Spotify, 2022).

-**Explicitness** is whether a song contains inappropriate words such as curse words and sexually explicit content that are unacceptable to play in some public settings. 1 is the value identifying a song as explicit, while 0 implies that a song is non-explicit. This is the dependent variable for the second research question (Spotify, 2022).

-**Acousticness** is a confidence measure from 0.0 to 1.0 of how much of the track is composed with acoustic instruments. 1.0 represents high confidence that the track is acoustic (Spotify, 2022).

-**Danceability** is a rating of a track's suitability for dancing. This metric is based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable (Spotify, 2022).

-**Energy** is a perceptual measure of intensity and activity. Energetic tracks typically feel fast, loud, and noisy (Spotify, 2022).

-**Instrumentalness** pertains to whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumentals in this context, while Rap or spoken word tracks are considered "vocal". If the instrumentalness values is greater than or equal to 0.5, the track is very likely to have no vocal content (Spotify, 2022).

-**Tempo** refers to the overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece, which derives directly from the average beat duration (Spotify, 2022).

-**Loudness** measures the overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological association of physical strength (amplitude). The values typically range between -60 and 0 db (Spotify, 2022).

-**Speechiness** detects the presence of spoken words in a track. The more speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attributed value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks (Spotify, 2022).

<ul>

**Relationship of interest and Justification of variables**

In this research we sought to understand which musical attributes gauged the popularity of songs in the 2010s using acousticness, danceability, energy, instrumentation, tempo, loudness, and speechiness as predictors and to what extent could the danceability, energy, speechiness, and tempo of a song predict whether it was explicit or non-explicit? 
The different variables combinations for the relationships of interest were choosen based on prior, domain knowledge of music. For the first research question, which concerns the features that popularized songs during the 2010s, a series of aforementioned predictors were chosen. What will follow is the justification for this "a priori" selection for both research questions.

Choosing the specific predictors to predict song popularity is due to the weight of their importance for the research question 1. Popular songs, whether they are an emotional ballad or a dance track, all have certain features to keep the listeners engaged and interested to repeat listening to these tracks (Leviatan, 2017). The tempo, energy and loudness indicate the pacing and sonic impact and pleasantness of the track. The speechiness, danceability and instrumentation (which also includes acoustic choices or "acousticness") dictate melody choices, chord progressions, instrument choices, wordings, vocal lines and more types sonic layers. However, the latter is very nuanced because it pertains to the genre choice of the producers. There are very popular songs with high instrumentation, no words and low danceability, such as songs from classical music. On the other hand, Pop and Rock songs vary their levels of instrumentation and acousticness and speechiness. Lastly, if the song is aimed towards a festive audience, such as a club song, then prioritizing danceability governs the levels of instrumentation and speechiness and lack of acousticness, and this prioritization varies by genre (Androids, 2017). In conclusion, the interplay of these factors influences the popularity of songs by making them memorable and enticing. 

As for the second research question, the explicitness of tracks is strongly swayed by other factors. A very logical approach to predicting explicitness was first looking at the high levels of speechiness in songs. For example, rap songs rank high in this metric because the verses are composed of a spoken word format over a series of 8 or 16 bars, and each bar is a rap line (Edwords), while singing doesn't have to adhere to the "1 bar = 1 line" rule; thus, speechiness became the metric of most importance. In addition, songs in this genre tend to include explicit content, often sexual, in the lyrics (Tayag, 2017). Second to this metric, the other predictors of danceability, energy and tempo were considered as helpful in predicting explicitness. The energy and danceability of the song collude with speechiness to infer if a track could have explicit language. For example, a song with low energy and low danceability may or may not be less likely to have explicit language than a song with high energy and danceability, holding the speechiness level constant, and this is a relationship we wish to investigate as well. As for tempo, music genres that are known to include explicit language follow specific tempos. For instance, Trap songs usually have a tempo of 140 bpm (Burchell, 2019).

##  Methods

### Data

The original data set was cleaned so that only tracks and variables relevant to the relationships of interest remained. To isolate the songs from the 2010 decade, we extracted all of the release years from the year column. To distinguish songs from podcasts and other audio, we used the speechiness and tempo metrics. Speech tracks with speechiness values less than 0.66 are podcasts and poetry, and tracks with tempo values of 0 are real songs with missing tempo values or tracks of rain sounds and white noise. Following the completion of these cleaning processes, our final subset contained a total of 104,767 records. We then used charts, graphs, and a table to provide a detailed examination of each of the variables in relation to the research questions.

### Models

For the relationship between popularity and acousticness, danceability, energy, instrumentation, tempo, loudness, and speechiness, we used multiple linear regression. Popularity the response variable was regressed on the predictors. The predictors were mean centered to improve the interpretability of the model results with an increase in any predictor causing a change to the average popularity of a song. Additionally, predictors energy, danceability, speechiness and instrumentalness were scaled because their values are recorded with higher precision. The seven predictors were put through a backward selection process with AIC to determine the best indicators of popularity. We choose, AIC as it imposes a lower penalty for having multiple independent variables and were seeking to maximize the possibility of unknown relationships in our data set. The model with the lowest AIC was selected and fitted.

For the relationship between explicitness and danceability, energy, speechiness, and tempo, we chose to use logistic regression because explicitness was a categorical variable that took on a binomial distribution, in this case, a song is either explicit or non-explicit. The explicitness variable was factored 1 for explicitness and 0 for  non-explicitness. The data was split between a train set using tracks released from 2010-2017 and tested on the test set using tracks released from 2018 to 2019. 

###  Model Assessment

The linear regression was checked for the assumptions of linearity, equal variance and independence of error terms, and normality of residuals. Using cooks distance we determined and removed influential points and the model was refit to accommodate the changes. A Variance Inflation Factor (VIF) was run to investigate if there was multicollinearity in the model, the model coefficients, p values, t values and confidence intervals were assessed and the model results were interpreted.

*The logistic regression  Then the process concludes with three final steps, assessing the significance of the predictors in the model as well as their insights, checking potential multicollinearity by VIF scores, and evaluating the model's accuracy. For the first, the coefficients of our predictors indicate the odds ratio that our song may be explicit as compared to inexplicit. For the second, we verify that our variables are not influencing each other too much because they are assumed to be independent of each other. For the third, the model generated from the train set is used to predict the likelihood of a song being explicit or non-explicit in the test set, which is the final step in logistic regression (cite). We then gauge the effectiveness of our model by observing the general accuracy values and the Area Under the Curve (AUC) value, another measure of how well the model is predicting the negatives and positives in a study.After the regression model is fit, we shall check for multicollinearity using a VIF test. If there is at least one predictor with a score higher than 5, it suggests high multicollinearity, and the model shall be refit without that predictor. At this stage, we shall also examine the binned residuals plot, to verify that the data falls within the band of a 95% confidence interval. If the majority of the points are not contained within those bands, an appropriate transformation will be made. Then, we shall assess independence. Lastly, a deviance test between the null and final model will be conducted to assess the quality of the overall fit.To evaluate our result from logistic regression, we shall calculate the accuracy of our predictive probabilities, plot the ROC curve and generate the AUC value. We will then be able to know how well the model predicted the probabilities of explicitness and non-explicitness (1 vs 0) for the out-of-sample, test data (year 2018-2019). After this initial assessment, we will create a confusion matrix and plot the ROC to obtain the AUC value, the final measure of how well the model is classifying positives and negatives.*

## Results

### Exploratory Data Analysis

To fully understand the data we worked with, we explored the basic statistics of the variables of interest in the table below. 

```{r , echo=FALSE,results="asis", header=FALSE, message=FALSE, warning=FALSE }

table1(~ acousticness+danceability+energy+instrumentalness+tempo+loudness+speechiness+popularity| explicit_fac, data=subset, flip_data=TRUE, overall = "total")
```
From the table, we observed that the mean, median, standard deviation, minimum and maximum values of variables categorized by explicit and non explicit. While mean values of acousticness, instrumentalness, tempo are higher for non explicit songs, values for energy, danceability, loudness and speechiness were higher for explicit songs. In addition, we could infer that popularity of explicit songs is quite higher than non explicit songs. High values for standard deviation for tempo and popularity indicate that the data points were spread out in relation to the mean value, whereas low values for danceability, energy, speechiness indicate that the data points were clustered around the mean. Lastly, nearly equal values of median and mean for danceability, energy and tempo indicate that the data points were more or less evenly distributed.

We used correlation matrix (*see Appendix*)to ascertain the weight of association amongst the predictors and the variables were not strongly corrleated except for acousticness and energy, which had a fairly strong correlation, something that we looked out for in the modeling process. 

```{r , echo=FALSE,results="asis", header=FALSE, message=FALSE, warning=FALSE}
RQ1_relation <- c("popularity", "acousticness", "danceability", "energy", "instrumentalness", "tempo", "loudness", "speechiness")
df1 = subset[RQ1_relation]
RQ2_relation <- c("explicit_fac", "danceability", "energy", "speechiness","tempo")
df2 = subset[RQ2_relation]
```

**Exploratory Data Analysis for the variables**

To better understand the data set, all songs popularity below 80 were compared to those with popularity above 80, ideally those between 100 and 80 popularity index score.(*see Appendix*)

```{r, echo = FALSE, message=FALSE, results = FALSE }
#create data sets for EDA
#5 pop groups across the data for better grouping
edadf1<- df1%>%mutate(popularity_fac= case_when(popularity >= 20 ~ "Top group > 80",
                                                popularity <= 80 ~ "Top group < 80"))

#eda dataset for 2: includes release year
q2 <- c("explicit_fac", "danceability", "energy", "speechiness","tempo", "release_year")
edadf2 = subset[q2]

```

```{r echo=FALSE, message = FALSE, out.width = "50%", results = "hide", fig.show='hold'}
#Relationship between danceability and popularity
ggplot(edadf1) +
  aes(x = "", y = danceability, colour = popularity_fac) +
  geom_boxplot(fill = "#112446") +
  scale_color_hue(direction = 1) +
  labs(
    x = "Popularity",
    y = "Danceability",
    title = "Relationship between danceability and popularity",
    color = "Popularity Groups",
    caption = "Fig 1"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5))

#Relationship between Instrumentalness and energy along the popularity scale
ggplot(edadf1) +
  aes(x = instrumentalness, y = energy) +
  geom_point(shape = "circle", size = 1.5, colour = "#F8766D") +
  labs(
    x = "Instrumentalness",
    y = "Energy",
    title = "Instrumentalness and Energy along the popularity scale",
    caption = "Fig 2"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
  facet_wrap(vars(popularity_fac))
```

In Figure 1, we observe that more popular songs have a higher average danceability than less popular songs. 
In Figure 2, we observe that, as songs become more popular, the energy remains evenly distributed, but the instrumentalness is reduced with the exception of a few outliers, although the relationship between the two variables becomes insignificant.

```{r echo=FALSE, message = FALSE, out.width = "50%", results = "hide", fig.show='hold'}
#Relationship between loudness and tempo along the popularity scale
ggplot(edadf1) +
  aes(x = loudness, y = tempo) +
  geom_point(shape = "circle", size = 1.5, colour = "#00BA38") +
  labs(
    x = "loudness",
    y = "tempo",
    title = "Loudness and Tempo along the popularity scale",
    caption = "Fig 3"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
  facet_wrap(vars(popularity_fac))

#Relationship between speechiness and instrumentalness along the popularity scale
ggplot(edadf1) +
  aes(x = speechiness, y = instrumentalness) +
  geom_point(shape = "circle", size = 1.5, colour = "#619CFF") +
  labs(
    x = "speechiness",
    y = "instrumentalness",
    title = "Speechiness and Instrumentalness along the popularity scale",
    caption = "Fig 4"
  )+
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
  facet_wrap(vars(popularity_fac))

```

For Figure 3, the relationship between tempo and loudness reveals a very specific recipe for the most popular songs. This group's tempo lies between 50-200 BPM (beats per minute) and -20 to 0 db, while other groups' tempo and loudness profiles remain spread out across the axes of tempo and loudness. Similarly, for Figure 4, although the speechiness values remain evenly distributed, the most popular songs exhibit a massive reduction in values in instrumentation.

Overall, the insights show that there seems to be an assortment of musical attributes that define an extremely popular song, yet they seem to be less pronounced for the less popular groups of songs. In other words, the most popular song profiles demonstrate an absence of instrumentalness and a specific range of tempo and loudness values.

*RQ2*
For Figure 5, it is clear that the explicit content in music has risen over the past decade. As for Figure 6, the energy in explicit songs is centered around a mean of 0.680, while the non explicit songs are skewed to a higher energy metric. In terms of Figure 7, the danceability of songs are centered around the mean of 0.687 for explicit songs and 0.599 for non-explicit songs. As for Figure 8, on average, explicit songs are less speechy than non-explicit songs. In conclusion, the chosen variables would give us a great classification of explicit and non explicit songs moving forward. 

```{r echo=FALSE, message = FALSE, out.width = "50%", results = "hide", figures-side, fig.show='hold'}
#explicity over the years
ggplot(edadf2) +
  aes(x = release_year, colour = explicit_fac) +
  geom_density(adjust = 1L, fill = "#112446") +
  scale_color_manual(
    values = c(`Non-Explicit` = "#440154",
    Explicit = "#FDE725")
  ) +
  labs(
    title = "Explicit and non explicit content in music over the years",
    x = "Release year",
    y = "Density of explicit content in music ",
    caption = "Fig 5"
  )+
  scale_x_continuous(breaks=seq(2010,2019,1))+
  theme_minimal()+
  theme(plot.title = element_text(face = "bold", hjust = 0.5))

#energy in explicit music
ggplot(edadf2) +
  aes(x = energy, colour = explicit_fac) +
  geom_histogram(bins = 30L, fill = "#112446") +
  scale_color_manual(
    values = c(`Non-Explicit` = "#440154",
    Explicit = "#FDE725")
  ) +
  labs(
    title = "Energy in explicit and non explicit songs",
    caption = "Fig 6")+
  theme_minimal()+
  theme(plot.title = element_text(face = "bold", hjust = 0.5))

#danceability in explicit music
ggplot(edadf2) +
  aes(x = danceability, fill = explicit_fac) +
  geom_histogram(bins = 30L) +
  scale_fill_manual(
    values = c(`Non-Explicit` = "#440154",
    Explicit = "#FDE725")
  ) +
  labs(
    title = "Danceability in explicit and non explicit songs",
    caption = "Fig 7")+
  theme_minimal()+
  theme(plot.title = element_text(face = "bold", hjust = 0.5))

#speechiness in explicitt music
ggplot(edadf2) +
  aes(x = "", y = speechiness, fill = explicit_fac) +
  geom_boxplot() +
  scale_fill_manual(
    values = c(`Non-Explicit` = "#440154",
    Explicit = "#FDE725")
  ) +
  theme_minimal() +
  labs(
    title = "Speechiness in explicit and non explicit songs",
    
    caption = "Fig 8")+
  theme_minimal()+
  theme(plot.title = element_text(face = "bold", hjust = 0.5))

```

### Model Results

**Linear Regression Model**

```{r subset, echo = FALSE, message=FALSE, results = FALSE}
#Load the data
df1 <- read.csv("https://raw.githubusercontent.com/EricR401S/Modeling-Trends-for-Spotify-Songs/part2/archive/rq1.csv")

#divide first
df1$danceability0.1 <- (df1$danceability)*10
df1$speechiness0.1 <- (df1$speechiness)*10
df1$energy0.001 <- (df1$energy)*1000
df1$instrumentalnes0.001 <- (df1$instrumentalness)*1000


#centered
df1$acousticness_centered <- scale(df1$acousticness, scale = FALSE)
df1$danceability0.1_centered <- scale(df1$danceability0.1, scale = FALSE)
df1$energy0.001_centered <- scale(df1$energy0.001, scale = FALSE)
df1$instrumentalness0.001_centered <- scale(df1$instrumentalnes0.001, scale = FALSE)
df1$tempo_centered <- scale(df1$tempo, scale = FALSE)
df1$loudness_centered <- scale(df1$loudness, scale = FALSE)
df1$speechiness0.1_centered <- scale(df1$speechiness0.1, scale = FALSE)

#backward
scaled1 <- lm(popularity ~ acousticness_centered+danceability0.1_centered+energy0.001_centered+instrumentalness0.001_centered+tempo_centered+loudness_centered+speechiness0.1_centered, data = df1)
scaled <- stepAIC(scaled1, trace=FALSE, direction = 'backward')

#Remove outliers and influential points
cooksd <- cooks.distance(scaled)
# influential row numbers
influential <- as.numeric(names(cooksd)[(cooksd > (4/nrow(df1)))])
df1_noinfluential <- df1[-influential, ]

model_noinfluential <- lm(popularity ~ acousticness_centered+danceability0.1_centered+energy0.001_centered+instrumentalness0.001_centered+tempo_centered+loudness_centered+speechiness0.1_centered, data = df1_noinfluential)

summ(model_noinfluential, digits = 7)
```
From the above model results, acousticness, danceability, energy, instrumentalness, loudness and speechiness are all significant predictors of a song's popularity score at the $\alpha = 0.05$ significance level ($p< 0.005$). At mean popularity of 39 a song has `r round(mean(df1$acousticness), 2)`,`r round(mean(df1$danceability),2)`, `r round(mean(df1$energy),2)`, `r round(mean(df1$instrumentalness),2)`, `r round(mean(df1$tempo),2)`, `r round(mean(df1$loudness),2)` and `r round(mean(df1$speechiness),2)`. For every 1 unit increase in acousticness and loudness, a song's average popularity will increase by 1.06 and 1.6 respectively, for every 0.1 unit increase in danceability and speechiness, the song's average popularity will increase by 1.13 and 1.14 respectively and for every 0.001 increase in a songs energy and instrumentalness the song's average popularity will decrease by 0.03 respectively in all cases holding all other factors constant. Additionally the model fit has $R^2$ value of 0.3059 meaning almost 31% of the change in popularity of a song is explained by these sonic characteristics.

We are 95% confident that the intervals in the table below contains the true value for the significant predictors.

```{r echo = FALSE, message=FALSE, results = FALSE }
confinterval_scaled <- confint(scaled, level = 0.95)
kable(confinterval_scaled, caption =  "Confidence interval ranges for popularity model") %>% kable_styling(position="center",latex_options = c("hold_position"))
```

The predictors in this model were found to be important in the order visualized below.

```{r echo = FALSE, message=FALSE, results = FALSE , out.width = "50%", fig.align='center'}
tvalues <- data.frame(summary(model_noinfluential)$coefficients[,3])
name <- c("acousticness","danceability","energy","instrumentalness","tempo","loudness","speechiness")
tdf <- data.frame(cbind(name, tvalues[2:8,]))

```

**Logistic Model**

```{r echo = FALSE, message=FALSE, results = FALSE}
r2 <- glm(explicit_fac ~ ., data = train , family = binomial(link = logit))
stargazer(exp(cbind(OR = coef(r2))), type = "latex", report = ("vcsp*"),header = FALSE, single.row = TRUE, digits = 4, no.space = TRUE, column.sep.width = "3pt", title = "Odds Ratio : Logistic Regression Model for Explicitness")
```

From the model results above, the model predictors are all statistically significant in predicting a song's explicitness at $p< 0.005$.In terms of interpreting the coefficients, holding all other variables constant, the odds of a song being explicit compared to being non-explicit is 1.022 times for every percentage unit increase in the danceability; 1.006 times for every percentage unit increase in the energy predictor; 1.09 times for every percentage unit increase in the speechiness; 0.996 times for every unit increase in the tempo.

```{r, echo=FALSE, message = FALSE, out.width = "50%", results = "hide", fig.show='hold'}
ggplot( train, aes(x = explicit_fac, y= danceability, fill = explicit_fac ) ) + 
  geom_boxplot() + coord_flip() + scale_fill_brewer(palette = "Blues") +
  labs( title = "Figure 1. Danceability and Explicitness" ) + theme_classic() + theme(legend.position = "none")

ggplot( train, aes(x = explicit_fac, y= speechiness, fill = explicit_fac ) ) + 
  geom_boxplot() + coord_flip() + scale_fill_brewer(palette = "Blues") +
  labs( title = "Figure 2. Speechiness and Explicitness" ) + theme_classic() + theme(legend.position = "none")

ggplot( train, aes(x = explicit_fac, y= tempo, fill = explicit_fac ) ) + 
  geom_boxplot() + coord_flip() + scale_fill_brewer(palette = "Blues") +
  labs( title = "Figure 3. Tempo and Explicitness" ) + theme_classic() + theme(legend.position = "none")

ggplot( train, aes(x = explicit_fac, y= energy, fill = explicit_fac ) ) + 
  geom_boxplot() + coord_flip() + scale_fill_brewer(palette = "Blues") +
  labs( title = "Figure 3. Energy and Explicitness" ) + theme_classic() + theme(legend.position = "none")
```

From the bar graphs above, we can see that only tempo has a slightly negative effect on explicitness (on average, the more speedy, the less explicit), while the speechiness, danceability, and tempo are positively related to explicitness. More specifically, we can see speechiness and danceability have stronger relationship to explicitness as compared to tempo and energy.
Even though every predictor in the analysis has been proved to be statistically significant, showing the evident association between the predictors and the explicitness, when we look at the size of odds ratio, it is however not that impactful. This makes sense given that a music is consist of many elements and only enhancing one aspect should not have a huge effect on explicitness. For instance, if danceability increases by 10%, it will only make the odds of being explicit increase 6% (1.006**10 = 1.06). In other words, very danceable records are more likely have explicit lyrics, while the impact isn't strong. Energy also implied a similar relationship. As for speechiness, its extremely high odds are best understood by thinking "the more words a song has, the more likely it is to have some explicit content". Out of expectation, tempo's odds ratio indicates that, as a song becomes slower, it will be more likely to be inexplicit lyrics. 


### Model assesment

**Checking Assumptions of the linear Model**
<ul>
* **Linearity**: The residuals vs predictor plot shows that there in no pattern between the two variables and as earlier remarked in Fig 2 there is a linear relationship between interval and duration.
* **Independence of error terms** : The points on the residual vs fitted plot are randomly spread so there was no clear violation of this assumption.
* **Equal Variance of error terms** : The points on the residual vs fitted plot are equally spread around the ab line(0,0) therefore we can conclude that there is homoscedasticity.
* **Normality of residuals**: The points on the quantile-quantile plot are clustered along the 45 degree line so there are no clear violations of this assumption.
<ul>
In conclusion, the assumptions of the model do not not appear to be strongly violated.

**Multicollinearity in the linear model**

The model predictors were tested for multicollinearity using the Variance Inflation Factor. In the table below we observed that there was no multicollinearity as all the VIF scores were below 5.
```{r echo = FALSE, message=FALSE, results = FALSE}
vif1 <- vif(model_noinfluential)
kable(vif1, caption =  "VIF Results for linear regression") %>% kable_styling(position="center",latex_options = c("hold_position"))
```
**Outliers and Influential points**

We checked the data for outlier and influential points and compared the effect of removing them on the model. We used Cook’s Distance to determine how much influence an observation has on the model with a higher distance meaning more influence as it uses the leverage and the residuals to calculate the score. All points having distance above 4/N, where N is the sample size, are influential points were excluded from the data points bringing the data set to 9838 observations. 
After removing the influential points the model R^2 increased from about 17% to 31% and the tempo predictor became insignificant.

```{r echo = FALSE, message=FALSE, results = FALSE, fig.show="hold", out.width="50%", fig.align = "center"}
cooksd <- cooks.distance(scaled)
# Plot the Cook's Distance using the traditional 4/n criterion
sample_size <- nrow(df1)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4/sample_size, col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")  # add labels
```
**Multicollinearity in the logistic model**

The model predictors were tested for multicollinearity using the Variance Inflation Factor. In the table below we observed that there was no multicollinearity as all the VIF scores were hardly above 1, which means the predictors were independent.

```{r echo = FALSE, message=FALSE, results = FALSE}
vif2 <- vif(r2)
kable(vif2, caption =  "VIF Results for logistic regression") %>% kable_styling(position="center",latex_options = c("hold_position"))
```

**Testing the model**
Lastly, we looked at the final and most important piece, the accuracy of our model on the test set. Shown below are the ROC curve and the confusion matrix, as well as the overall accuracy values. One of our challenges was addressing the small number of explicit songs in our data, where it might hurt model accuracy. That concern was proven true when our model failed at correctly classifying 3,937 explicit songs. It had an overall, high accuracy of 81%, a sensitivity of 97.9% and a specificity of 14%. The sensitivity value signifies that our model is efficient at identifying non-explicit songs, while our specificity value demonstrates that our model is lacking when attempting to identify an explicit song. Also, the AUC value was a 79% success rate of distinguishing negatives and positives.

```{r warning=FALSE, echo=FALSE, message = FALSE, results="hide"}
test_fitted_0 <- predict(r2, newdata = test, type = "response") # Change Here
test_fitted <- ifelse(test_fitted_0 >= 0.5, "Explicit", "Non-Explicit") #Change here
Conf_mat3 <- confusionMatrix(as.factor(test_fitted), test$explicit_fac)
ConfMat3 <- as.data.frame.matrix(Conf_mat3$table)
Accuracy3 <- Conf_mat3$overall["Accuracy"]
stargazer(ConfMat3, summary = FALSE, type = "text", title = "Confusion Matrix")
stargazer(Accuracy3, type = "text", summary = FALSE)
invisible(roc(test$explicit_fac,test_fitted_0,plot=T,print.thres=0.5,legacy.axes=T, print.auc =T,col="red3", main = "ROC Curve -- Logistic Regression Model : Explicitness")) # Inclusion here
```

Confusion Matrix table with labels (True Negatives, True Positives, we just need to know which one is which)

```{r, message = FALSE, fig.show="hold", out.width="50%", fig.align = "center"}
conf.mat.plot(as.factor(test_fitted), test$explicit_fac, main = "confusion matrix with label" )
```

## Conclusion

For our first research question we derive the conclusions that:
1. The  predictors *acousticness*, *danceability*, *energy*, *instrumentatalness*, *loudness*, and *speechiness* are statistically significant and are highly influential in determining the response variable *popularity* of the song in the 2010s decade. *Tempo* however, is not a influential variable according to the model analysis.

2. With the exception of *energy* and *instrumentalness*, the *popularity* score for every track in the 2010s decade increases with increase in the value of the predictors. 

3. As all the VIF values are below 5, there is no significant multicolinearity observed in the model predictors. 

4. The model fit improves after removing the influential points, we observe the Adjusted-Rsquared value to increase from 0.1725 to  0.3059. This could be a potential limitation as even after removing influential points, the predictors can only explain variability in outcome to upto 30.59%. This indicates a possibility of other significant predictors which have not been taken into account in the analysis. For example popularity of a track would also be dependent largely on who the artist is.

### Key Insights

Analysis of the first research question, which dealt with inferring the significant predictors impacting the popularity of the song, we come to the conclusion that musical features like acousticness, danceability, energy, instrumentalness, loudness and speechiness are important attributes that impact the popularity of the song. A track that has high levels of acousticness, danceability, loudness and speechiness is likely to have a higher popularity score. While a track with low levels of energy and instrumentalness will have a lower popularity score. 


Based on the second research result, we can conclude that the four musical aspects, danceability, energy, speechiness and tempo, we chose are indeed related to explicitness and can predict explicitness to some extent. Overall, the model is a good fit with an acceptable prediction accuracy score (81%). More specifically, we found out that increasing danceability and speechiness would increase explicitness to certain extent, and thus for music producers who would like to avoid expliciitness, they would want to lower the danceability and the speechiness of the song.


### Limitations & Recommendations/Future Studies

Despite providing insights toward predicting explicitness, the analysis indeed has some limitations. First, our model is weak at identifying non-explicit songs, probably because there was a smaller distribution of explicit songs in out dataset, which is indicative of the pattern of music in the real world because the majority of songs are non-explicit; however, such distribution might cause the training process being rough. Secondly, more or better predictors may have been needed for this particular research question, such as what language of the song is. Thirdly, Spotify likely employs the use of speech recognition (natural language processing) models to flag the explicitness of songs, not statistical methods, meaning that they ignore most of the musicical aspects, such as tone, into account when classifying the explicitness. Thus, the research where we used the four predictors to predict such "latent" variable statistically may have presented difficulty at the beginning. To conclude, these results do prove that our model is insufficient for predicting the explicitness of songs (there are 19% of errors in predicting), but overall it is very successful when classifying the explicit songs. Future work needs to pay more attention to the fundamental causes of wrongly flagging non-explicit songs as explicit songs. 

#Appendix

## Binned Residual Plot

```{r warning=FALSE, echo=FALSE, message = FALSE, results="hide"}
library(performance)
binnedplot(fitted(r2), residuals(r2,"response"), xlab = "Predicted Probability")
results <- binned_residuals(r2)
as.data.frame(results) 
if (require("see")){plot(results)}
plot(results$xbar, results$ybar)
which(results$ybar < -0.1)
```
Overall, the binned residual plot shows that we have a good fit. However, we indeed need to be cautious about a few of outliers.From the residual binned plot, we can see that there are 5 outliers having residual larger than 0.1 difference from what was expected. Given that our sample size is huge enough, so we would not do further actions on these 5 outliers.


## Bibliography

Androids (2017, October 13). An Idiot’s Guide to EDM Genres. Retrieved October 20, 2022, from https://www.complex.com/music/an-idiots-guide-to-edm-genres/

Billboard. (2022, October 29). Billboard Hot 100. Billboard Media. Retrieved November 3, 2022. https://www.billboard.com/charts/hot-100/2022-10-29/

Burchell, C. (2019, May 27). 10 Tips for Making Your First Trap Beat. Inverse. Retrieved October 20, 2022, from https://flypaper.soundfly.com/produce/10-tips-for-making-your-first-trap-beat/#

Edwords, E. (n.d.). Rap Song Structure Is TOO Important To Ignore. Retrieved October 20, 2022, from https://rhymemakers.com/rap-song-structure/

Leviatan, Y. (2017, July 27). Making Music: The 6 Stages of Music Production. Waves. Retrieved October 20, 2022, from https://www.waves.com/six-stages-of-music-production

Spotify (2022). Spotify Web API Reference | Spotify for Developers. https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features

Tayag, Y. (2017, May 17). Expert on Male Psychology Explains How Pop Got Sexually Explicit. Retrieved October 20, 2022, from https://www.inverse.com/article/31842-pop-music-sexually-explicit-lyrics-rap-hip-hop

Yamac. (2016). Spotify Dataset 1921-2020, 600k+ Tracks. Spotify. Retrieved October 3, 2022, from https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks

