---
title: "IDS 702 Team Black Project"
author: "Emma Wang, Pragya Raghuvanshi, Lorna Aine, Eric Rios Soderman"
date: "2022-10-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


Load the data

```{r subset}
#read data
#df <- read.csv("/Users/pr158admin/Modeling-Trends-for-Spotify-Songs/archive/tracks.csv")
df <- read.csv("https://github.com/EricR401S/Modeling-Trends-for-Spotify-Songs/raw/main/archive/tracks.csv")
```

Cleaning

# Two columns of interest : Name of artist (if one wanted to highlight certain summary statistics), duration conversion (get minutes), the rest is perfect on its own. 


```{r}

# Confirming the data types of the columns
sapply(df, class)

# Removing the brackets from the names of the artists
# This will facilitate summary statistics

df$artists<-gsub("]","",as.character(df$artists))
df$artists<-gsub("^.","",as.character(df$artists))

# New minute variable for our own use to simplify interpretation

df$duration_minutes <- df$duration_ms/(1000*60)


# Confirming that there are no missing data, except the artist name column

colSums(is.na(df))

# change "explicit" into binary factor
df$explicit_fac <- factor(df$explicit,
                         levels=c(0,1),
                         labels=c('Non-Explicit','Explicit'))

# make Date readable to R
# Emma and Eric need to revise this together, some records with Dates passed as NAs in R after running this code.
# That's the theory. It was single year dates.
df$release_date <- as.Date(df$release_date, "%Y-%m-%d")


# According to variable definitions, speechiness levels above 0.66 are speech tracks such as podcasts and poetries.
# They will be removed.


df0 <- df[df$speechiness <= 0.66,]
nrow(df) - nrow(df0) #22,598 records of speech tracks

# Examining records with a value of 0 for tempo
# A total of 328 records with 0 tempo were found, and most were tracks of rain sounds and introductions. 
# Some were, unfortunately, real songs.

sum(df0$tempo==0)

# 148 of those records are from the 2010s decade, our area of interest. 
tempo_0_subset <- df0[df0$tempo == 0,]
tempo_0_subset_2010s <- tempo_0_subset[grep('201[0-9].*', tempo_0_subset$release_date),]
nrow(tempo_0_subset_2010s)

# Removing records with a value of 0 for tempo

df0 <- df0[df0$tempo != 0,]


```

```{r subset}
#subset data
subset <- df0[grepl('201[0-9].*', df$release_date),]
#subset <- df0[grep('2010.*', df$release_date),]
```
## I.Data Overview 


Project Proposal : Provide the chief characteristics of your data, including sample size, number of
variables, and source. Include your research questions here.


Description of Dataset




Definitions of Main Variables of interest

*Popularity (DV1)* is calculated by an algorithm that is based on how many times a track has been played and how recent those plays were. This is the response variable of interest for research question 1 (Spotify, 2022).

*Explicitness (DV2)* is whether a song contains inappropriate words such as curse words and sexually explicit words that are unacceptable to play in some public settings. 1 is the value identifying a song as explicit, while 0 implies that a song is non-explicit. This is the dependent variable for the second research question (Spotify, 2022).

*acousticness* is a confidence measure from 0.0 to 1.0 of how much of the track is composed with acoustic instruments. 1.0 represents high confidence the track is acoustic (Spotify, 2022).

*danceability* is a rating of a track's suitability for dancing. This metric is based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable (Spotify, 2022).

*energy* is a perceptual measure of intensity and activity. Energetic tracks typically feel fast, loud, and noisy (Spotify, 2022).

*instrumentation* pertains to whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumentals in this context, while Rap or spoken word tracks are considered "vocal". As the instrumentalness value approaches 1.0, there is a greater likelihood that the track contains no vocal content. In addition, values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0 (Spotify, 2022).

*tempo* refers to the overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece, which derives directly from the average beat duration (Spotify, 2022).

*loudness* measures the overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological association of physical strength (amplitude). Lastly, the values typically range between -60 and 0 db (Spotify, 2022).

*speechiness* detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks (Spotify, 2022).


## II. Primary relationship of interest

Project Proposal Description - Present descriptive statistics and exploratory plots in whichever
format you think is best (tables, figures) for your primary relationship of interest (dependent variable
and primary independent variable, if applicable). Describe your findings.

EDA (graphs)



Relationship to Research Questions

First and foremost, we choose the variables for our research questions based on prior, domain knowledge of musical terminology. For the first research question, which concerns the features that popularized songs during the 2010s, a series of aforementioned predictors were chosen. What will follow is the justification for this "a priori" selection for both research questions.

The reasoning for choosing these specific predictors (acousticness, danceability, energy, instrumentation, tempo, loudness, and speechiness) to predict song popularity is the weight of their importance. Popular songs, whether they are an emotional ballad or a dance track, all have certain features to keep the listeners engaged and interested to repeat listening to these tracks (Leviatan, 2017). The tempo, energy and loudness indicate the pacing and sonic impact and pleasantness of the track. The speechiness, danceability and instrumentation (which also includes acoustic choices or "acousticness") dictate melody choices, chord progressions, instrument choices, wordings, vocal lines and more types sonic layers. However, this last point is very nuanced because it pertains to the genre choice of the producers. There are very popular songs with high instrumentation, no words and low danceability, such as songs from classical music. On the other hand, Pop and Rock songs vary their levels of instrumentation and acousticness and speechiness to deliver the best possible songs. Lastly, if the song is aimed towards a festive audience, such as a club song, then prioritizing danceability governs the levels of instrumentation and speechiness and lack of acousticness, and this prioritization varies by genre (Androids, 2017). In conclusion, the interplay of these factors influence the popularity of songs by helping them become easily memorable and enticing. 


As for the second research question, the explicitness of tracks is strongly swayed by other factors. A very logical approach to predicting explicitness was first looking at the high levels of speechiness in songs. For example, rap songs rank high in this metric because the verses are composed of a spoken word format over a series of 8 or 16 bars, and each bar is a rap line (Edwords), while singing doesn't have to adhere to the "1 bar = 1 line" rule; thus, speechiness became the metric of most importance. In addition, songs in this genre tend to include explicit content, often sexual, in the lyrics (Tayag, 2017). Second to this metric, the other predictors of danceability, energy and tempo were considered as helpful in predicing explicitness. The energy and danceability of the song collude with speechiness to infer if a track could have explicit language. For example, a song with low energy and low danceability may or may not be less likely to have explicit language than a song with high energy and danceability, holding the speechiness level constant, and this is a relationship we wish to investigate as well. As for tempo, music genres that are known to include explicit language follow specific tempos. For instance, Trap songs usually have a tempo of 140 bpm (Burchell, 2019).


In the research, we have two main Research Questions:
RQ1.	What are the musical attributes that gauged the popularity of songs in the 2010s? 
*Dependent Variable (continuous) = popularity*
*Independent Variables = acousticness, danceability, energy, instrumentation, tempo, loudness, and speechiness* 
In RQ1, we aim to "infer" what features of music could better measure the popularity.
 
RQ2.	To what extent can the musicality of a song predict whether a song will be explicit or non-explicit? 

*Dependent Variable (categorical) = Explicitness*
*Independent Variables = danceability, energy, speechiness, and tempo* 
In RQ2, we aim to "predict" whether the song is explicit or not by the four features of music: dancebility, energy, speechiness, tempo.



Relationships within data

```{r}
library(psych)

RQ1_relation <- c("popularity", "acousticness", "danceability", "energy", "instrumentalness", "tempo", "loudness", "speechiness")
df1 = subset[RQ1_relation]

pairs.panels(df1)
```

```{r}
RQ2_relation <- c("explicit", "danceability", "energy", "speechiness","tempo")
df2 = subset[RQ2_relation]

pairs.panels(df2)
```
## III.Other characteristics

Briefly describe other variables in the data. If there are many, do not list
them all. Rather, describe the types of variables that are present (e.g., “demographic information”)

A few variables such as key and time signature are part of this dataset, although they were not chosen as the predictors. Most of the remaining variables in this dataset pertain to the artist name, the song title, the modalities and key of a song, the duration and the release dates. Nonetheless, some variables were still very rich in information in terms of prediction. To illustrate, one could predict the scale of a song based on the popularity in addition to other predictors. 

Some were not chosen because they were differently coded, such as time signature, which has an extremely limited set of plausible values (lacks signatures like 6/8). In contrast, some offered little or irrelevant information. Liveness is one such case. It parametrizes a song's performance as a live or studio quality recording, and, given that the songs that play on the radio tend to be studio songs, we opted to not use this variable as a predictor.

## IV. Potential Challenges

Describe aspects of the data that may present challenges in the modeling stage.
For example, might certain categorical variables need to be collapsed? Is there a lot of missingness?
Could the size of the dataset present model selection challenges?


Eric's findings from cleaning :
Yes, there is a bit of missingness. Some dates have years, but not months and days recorded. This meant that we had to manually fix them or give them arbitrary values. Some artist names are missing, which is notable but currently not obstructive. We also do not know if some of the tempos of the higher songs were incorrectly recorded.  


## V. Bibliography (Citations)

Androids (2017, October 13). An Idiot’s Guide to EDM Genres. Retrieved October 20, 2022, from https://www.complex.com/music/an-idiots-guide-to-edm-genres/

Burchell, C. (2019, May 27). 10 Tips for Making Your First Trap Beat. Inverse. Retrieved October 20, 2022, from https://flypaper.soundfly.com/produce/10-tips-for-making-your-first-trap-beat/#

Edwords, E. (n.d.). Rap Song Structure Is TOO Important To Ignore. Retrieved October 20, 2022, from https://rhymemakers.com/rap-song-structure/

Leviatan, Y. (2017, July 27). Making Music: The 6 Stages of Music Production. Waves. Retrieved October 20, 2022, from https://www.waves.com/six-stages-of-music-production

Spotify (2022). Spotify Web API Reference | Spotify for Developers. https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features

Tayag, Y. (2017, May 17). Expert on Male Psychology Explains How Pop Got Sexually Explicit. Retrieved October 20, 2022, from https://www.inverse.com/article/31842-pop-music-sexually-explicit-lyrics-rap-hip-hop



